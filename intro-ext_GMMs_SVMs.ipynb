{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "17iOifVYzRIm"
   },
   "source": [
    "# 02 â€” Introduction (extended), GMMs and SVMs\n",
    "\n",
    "This notebook extends the introduction from the last notebook and continues with GMMs and SVMs.\n",
    "\n",
    "As before, \n",
    "1. add the unmodified notebook to a new (or the former) git-repo\n",
    "2. run all notebook cells in order and commit files it generates (if any)\n",
    "3. adjust the notebook according to the exercies, answer questions in the provided MD-cells, \n",
    "4. run all notebook cells in order\n",
    "5. commit the changed notebook\n",
    "6. commit files it generates (if any)\n",
    "\n",
    "upload a git-bundle of your repo to StudIP \\\n",
    "BearbeitetePraktikumsaufgaben/Praktikum09\\\n",
    "till __7.1.21 18:00__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iris dataset\n",
    "\n",
    "### Loading the data\n",
    "\n",
    "The [Iris dataset](https://archive.ics.uci.edu/ml/datasets/iris) is so popular that scikit-learn can load it out-of-the-box:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal length (cm)</th>\n",
       "      <th>sepal width (cm)</th>\n",
       "      <th>petal length (cm)</th>\n",
       "      <th>petal width (cm)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>6.7</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>6.3</td>\n",
       "      <td>2.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>6.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>6.2</td>\n",
       "      <td>3.4</td>\n",
       "      <td>5.4</td>\n",
       "      <td>2.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>5.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.1</td>\n",
       "      <td>1.8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)\n",
       "145                6.7               3.0                5.2               2.3\n",
       "146                6.3               2.5                5.0               1.9\n",
       "147                6.5               3.0                5.2               2.0\n",
       "148                6.2               3.4                5.4               2.3\n",
       "149                5.9               3.0                5.1               1.8"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn import datasets\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "X = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
    "X.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1b\n",
    "Plot the data of only the first two dimension, labelling the axes according to the plotted quantities. [This](https://stackoverflow.com/questions/13187778/convert-pandas-dataframe-to-numpy-array#54508052) might be helpful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## plot first two dimension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1c\n",
    "Plot the data of only the last two dimension, labelling the axes according to the plotted quantities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## plot last two dimension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1d\n",
    "Use the [mplot3d module](https://matplotlib.org/3.1.1/gallery/mplot3d/scatter3d.html) to create a 3D plot the data using the first three dimension, labelling the axes according to the plotted quantities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## static 3D plot with labelled axes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "basing on the previous notebook here, __focusing on labelling the axes in the plot__:\n",
    "\n",
    "### Principal Component Analysis\n",
    "\n",
    "As you can see, the dataset has 4 numerical features (dimensions). If we want to show all the information in a 2D plot, we have to perform a dimensionality reduction first. We will use [scikit-learn's implementation of PCA](http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html) for this.\n",
    "\n",
    "### Exercise 2\n",
    "\n",
    "Use PCA to transform the original data into 2 dimensions and show the transformed data in a scatter plot, with the colors of each data point corresponding to the `target` value.\\\n",
    "__Also annotate the axes with their labels again.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## do PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## create scatter plot of PCA output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2b\n",
    "Now create a 3D scatter plot again (in the order 2,0,1), enabling interactions with `%matplotlib notebook` ([see here](https://stackoverflow.com/questions/38364435/python-matplotlib-make-3d-plot-interactive-in-jupyter-notebook#38865534), [does not work in JupyterLab](https://stackoverflow.com/questions/51922480/javascript-error-ipython-is-not-defined-in-jupyterlab)) and orient the 3D view such that it resembles the 2D plot of the PCA.\\\n",
    "Then fix the 3D view to the found azimuth and elevation with [`view_init`](https://stackoverflow.com/questions/12904912/how-to-set-camera-position-for-3d-plots-using-python-matplotlib#12905458).\n",
    "\n",
    "### Question\n",
    "There are (at least) two reasons why the 3D viw can only look similar to the 2D PCA plot. Do you know why? Explain!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answers\n",
    "1. \n",
    "2. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## switch off interaction for the following plots\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3\n",
    "\n",
    "Now try to separate the clusters with scikit-learn's implementation of [K-Means](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html) once with the 2D data from the PCA and once with the original 4D data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## K-means on 2D PCA output, just copy your solution from last notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## K-means on orig 4D data, just copy your solution from last notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4\n",
    "\n",
    "Now try to separate the clusters with scikit-learn's implementation of [DBSCAN](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.DBSCAN.html) again once with the 2D data from the PCA and once with the original 4D data, (you may stop trying to find apropriate parameters after 10 minutes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## DBSCAN on 2D PCA output, just copy your solution from last notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## DBSCAN on orig 4D data, just copy your solution from last notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5\n",
    "\n",
    "Try if scaling the data to a common mean and standard deviation helps to separate the clusters with DBSCAN in 4D.\\\n",
    "Use the [StandardScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html)  for scaling and check the statistics of the scaled features with pandas [describe](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.describe.html).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## scale orig 4D data with StandardScaler and output the statistics of the scaled features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## DBSCAN on scaled 4D data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 6\n",
    "\n",
    "Now test a Gaussian Mixture Models (GMMs) ([GaussianMixture](https://scikit-learn.org/stable/modules/generated/sklearn.mixture.GaussianMixture.html)) and plot it side-by-side (as above for the blobs) with the actual labels (as color)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## apply a GMM assuming 3 clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "## scatter plot using the GMM labels side-by-side to a scatter plot with the original lables (as color)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machines (SVMs)\n",
    "\n",
    "All ML methods above did not make use of the labels (only used for coloring). \n",
    "Now make use of the labels of the dataset to train a supervised ML method, like an SVM. Use the following function [plot_predictions](https://nbviewer.jupyter.org/github/ageron/handson-ml2/blob/master/05_support_vector_machines.ipynb#Non-linear-classification\n",
    ") for visualizing the decision boundaries of the trained SVM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "## from https://nbviewer.jupyter.org/github/ageron/handson-ml2/blob/master/05_support_vector_machines.ipynb#Non-linear-classification\n",
    "def plot_predictions(clf, axes):\n",
    "    x0s = np.linspace(axes[0], axes[1], 100)\n",
    "    x1s = np.linspace(axes[2], axes[3], 100)\n",
    "    x0, x1 = np.meshgrid(x0s, x1s)\n",
    "    X = np.c_[x0.ravel(), x1.ravel()]\n",
    "    y_pred = clf.predict(X).reshape(x0.shape)\n",
    "    # y_decision = clf.decision_function(X).reshape(x0.shape)\n",
    "    plt.contourf(x0, x1, y_pred, cmap=plt.cm.brg, alpha=0.2)\n",
    "    # plt.contourf(x0, x1, y_decision, cmap=plt.cm.brg, alpha=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 7\n",
    "\n",
    "Try to understand how the function `plot_predictions` works and explain each active code line by adding a trailing comment.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 8\n",
    "\n",
    "Next fit the PCA data (2D) with a linear SVM. Use scikit-learn for that. If you get a \\\n",
    "`ConvergenceWarning: Liblinear failed to converge, increase the number of iterations`\\\n",
    "try e.g. `max_iter=1e6`, see: https://stackoverflow.com/questions/52670012/convergencewarning-liblinear-failed-to-converge-increase-the-number-of-iterati\n",
    "\n",
    "Plot the decision boundaries of the trained SVM together with the colored training points (as before).\n",
    "\n",
    "#### Quenstions\n",
    "1. What effect has changing the hyper-parameter C? (Try e.g. 1, 10, 100)\n",
    "2. How can you see from the plot, that a linear SVM. was used?\n",
    "\n",
    "#### Answers\n",
    "1. \n",
    "2. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## train SVM with PCA data\n",
    "\n",
    "## SVM here works OK without scaling because the PCA data is quite balanced\n",
    "\n",
    "## plot decision boundaries and training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 9\n",
    "\n",
    "Extend you SVM evaluation with `PolynomialFeatures` followed by a `StandardScaler`, for ideas how to do that look through:\\\n",
    "https://nbviewer.jupyter.org/github/ageron/handson-ml2/blob/master/05_support_vector_machines.ipynb#Non-linear-classification\n",
    "\n",
    "Plot the decision boundaries of the trained SVM together with the colored training points (as before).\n",
    "\n",
    "#### Quenstions\n",
    "1. What effect has changing degree of the `PolynomialFeatures`? (Try e.g. 1, 2, 3, 5, 10, 20)\n",
    "2. Why are we using the PCA data and not the original iris data?\n",
    "\n",
    "#### Answers\n",
    "1. \n",
    "2. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## train SVM with PolynomialFeatures (which probably need scaling)\n",
    "\n",
    "## plot decision boundaries and training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting the dataset for training and testing\n",
    "\n",
    "By holding back some of the data during the training of ML models, the remaining data can be used to evaluate the performance of the resulting model. This is often called testing. This has the disadvantage, that not all data can be used for training.\n",
    "\n",
    "### Exercise\n",
    "\n",
    "Use sklearn's `train_test_split` to split the iris dataset into a training dataset (75%) and a testing dataset (25%).\\\n",
    "Then train your linear and polynomial SVMs on the reduced training dataset and then check their performance with the `score()` function of each classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load and apply train_test_split\n",
    "\n",
    "## report the sizes of the resulting training and testing datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "## train and evaluate the resulting linear SVM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "## train and evaluate the resulting polynomial SVM model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To avoid problems from this *static* separation into train and test datasets, it is possible to divide the full dataset into e.g. 4 (nearly) equally sized subsets (called folds). Then it is possible to use each fold once as test set and the other folds for training, a kid of *dynamic* separation. This technique is called cross-validation (CV).\n",
    "\n",
    "### Exercise\n",
    "\n",
    "Scoring a classifier in sklearn with CV can be done with `cross_val_score`. Print the scores CV yields for the linear and the polynomial SVM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "## apply cross_val_score on both SVMs and report their scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Be aware, that the differing scores just arise from using the different training subsets!\n",
    "\n",
    "The ability to score differntly trained models also bings a way to test models differing in their hyper-parameters. This allows to \"automatically\" determin the best combination of a set of various hyper-parameters (called grid). This is called `GridSearchCV` in sklearn. Using only a random subset of this grid (to reduce the total computation load) is provided by `RandomizedSearchCV`. "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "01_Intro_ex.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
